\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{shamith2}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{lab-2-classification-part-1}{%
\section{Lab 2: Classification (Part
1)}\label{lab-2-classification-part-1}}

    \hypertarget{name-shamith-achanta-shamith2}{%
\subsection{Name: Shamith Achanta
(shamith2)}\label{name-shamith-achanta-shamith2}}

    \hypertarget{due-september-9-2020-1159-pm}{%
\subsubsection{Due September 9, 2020 11:59
PM}\label{due-september-9-2020-1159-pm}}

    \hypertarget{logistics-and-lab-submission}{%
\paragraph{Logistics and Lab
Submission}\label{logistics-and-lab-submission}}

    See the
\href{https://courses.engr.illinois.edu/ece365/fa2019/logisticsvvv.html}{course
website}.

    \hypertarget{what-you-will-need-to-know-for-this-lab}{%
\paragraph{What You Will Need To Know For This
Lab}\label{what-you-will-need-to-know-for-this-lab}}

    This lab covers some basic classifiers which can be used for M-ary
classification.

\begin{itemize}
\tightlist
\item
  Bayes Classifiers
\item
  Linear Discriminant Analysis
\item
  k-Nearest Neighbors
\end{itemize}

There are some problems which have short answer questions. Do not write
an essay -- a few (1-2) complete sentences will suffice.

Also, be clear about your answers. For example, if a question asks you
``Which classifier would you choose?'', be unequivocal about which
classifier you would choose (and why); as engineers, part of your job is
to make design decisions and justify them in comparison to the
alternatives.

    \hypertarget{preamble-dont-change-this}{%
\paragraph{Preamble (Don't change
this)}\label{preamble-dont-change-this}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}}\PY{k}{pylab} inline
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{spatial}\PY{n+nn}{.}\PY{n+nn}{distance} \PY{k}{as} \PY{n+nn}{dist}
\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Populating the interactive namespace from numpy and matplotlib
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Load the data needed for Problems 1\PYZhy{}3 }

\PY{c+c1}{\PYZsh{} Read the data}
\PY{n}{traindata\PYZus{}tmp}\PY{o}{=} \PY{n}{genfromtxt}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{valdata\PYZus{}tmp}\PY{o}{=} \PY{n}{genfromtxt}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}The data which you will use to train LDA and kNN is called \PYZdq{}trainingdata\PYZdq{}}
\PY{n}{trainingdata}\PY{o}{=}\PY{n}{traindata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
\PY{c+c1}{\PYZsh{}The corresponding labels are in \PYZdq{}traininglabels\PYZdq{}}
\PY{n}{traininglabels}\PY{o}{=}\PY{n}{traindata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}

\PY{c+c1}{\PYZsh{}The data which you will use to validate LDA, kNN and the Bayes Classifier}
\PY{c+c1}{\PYZsh{}is called \PYZdq{}valdata\PYZdq{}}
\PY{n}{valdata}\PY{o}{=}\PY{n}{valdata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
\PY{c+c1}{\PYZsh{}The corresponding labels are in \PYZdq{}vallabels\PYZdq{}}
\PY{n}{vallabels}\PY{o}{=}\PY{n}{valdata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Some code to visualize decision regions in Problem 1 to 3; you don\PYZsq{}t need to look at this}
\PY{n}{adp}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{trainingdata}\PY{p}{,}\PY{n}{valdata}\PY{p}{]}\PY{p}{)}
\PY{n}{xmin}\PY{p}{,}\PY{n}{xmax} \PY{o}{=} \PY{n}{adp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{adp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}
\PY{n}{ymin}\PY{p}{,}\PY{n}{ymax} \PY{o}{=} \PY{n}{adp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{adp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1}
\PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{xmin}\PY{p}{,} \PY{n}{xmax}\PY{p}{,} \PY{l+m+mf}{0.05}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{ymin}\PY{p}{,} \PY{n}{ymax}\PY{p}{,} \PY{l+m+mf}{0.05}\PY{p}{)}\PY{p}{)}
\PY{n}{drdata}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{c\PYZus{}}\PY{p}{[}\PY{n}{xx}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{problem-1-bayes-classifiers-20-points-5-bonus-points}{%
\paragraph{Problem 1 : Bayes Classifiers (20 points + 5 bonus
points)}\label{problem-1-bayes-classifiers-20-points-5-bonus-points}}

    In this section, you will implement a Bayes classifier for the following
\(M\)-ary classification problem:

\[H_y: \mathbf{X} \sim \mathcal{N}(\mathbf{\mu}_y,{\sf C}) \qquad y=0,\ldots,M-1\]

i.e.~the data is a \(d\)-dimensional Gaussian with a common covariance
matrix \(\sf C\) among all classes, but the means are different (and
there is a prior among the classes). Remember, when the mean vectors,
covariance matrix and prior probabilities are known, no classifier can
do better than the Bayes classifier.

You will write a function which takes in 4 parameters: * A set of data
to classify (with rows as feature vectors) as a \((V,d)\) numpy.ndarray
(data) * A M-length vector with the prior probabilities of each class as
a numpy.ndarray (pi) * A matrix with rows giving the class means as a
\((M,d)\) numpy.ndarray (means) * The common covariance matrix as a
\((d,d)\) numpy.ndarray (cov)

It will output a length \(V\) numpy.ndarray of the outputs of the
classifier (labels). You may not use scikit-learn or similar to
implement this. Note that the class labels in this problem are \(0,1,2\)
(not \(1,2,3\)). Since Python uses zero-based indexing, this will allow
you to avoid a few +1's in your code.

Note that there are 5 bonus points for not using loops in Problem 1.

Some hints * If you did lab 1, exercises 5 and 6, they will get you
through the bulk of this problem. * A non-exhaustive list of useful
functions: numpy.linalg.inv, numpy.sum, numpy.log, numpy.argmax. * You
may use broadcasting to help simplify your code. The basic form you may
want to use is, if you have code which says A + B where A is (n,m) and B
is (m,) then numpy will automatically translate this to adding B to each
row of A.

A function prototype is provided below (10 points):

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{bayesClassifier}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{pi}\PY{p}{,}\PY{n}{means}\PY{p}{,}\PY{n}{cov}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Put your code here}
    \PY{c+c1}{\PYZsh{} number of samples in data and dimensions of feature vector}
    \PY{n}{n}\PY{p}{,} \PY{n}{d} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{shape}
    \PY{c+c1}{\PYZsh{} number of labels in data}
    \PY{n}{l} \PY{o}{=} \PY{n}{means}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{c+c1}{\PYZsh{} predictions}
    \PY{n}{pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} x = data \PYZhy{} mean (with distinct labels)}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{subtract}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{means}\PY{p}{,} \PY{n}{l}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} c = inv(cov)}
    \PY{n}{c} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{cov}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} nc := normalizing constant}
    \PY{n}{nc} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.25} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{p}{,} \PY{n}{d}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{cov}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} p := p(class|data)}
    \PY{c+c1}{\PYZsh{} p(class|data) = pi * p(data|class)}
    \PY{c+c1}{\PYZsh{} log(p(class|data)) = log(pi) + log(p(data|class))}
    \PY{c+c1}{\PYZsh{} log(p(class|data)) = np.log(pi) + nc \PYZhy{} (0.5 * (x.T * c * x))}
    \PY{n}{p} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{pi}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{l}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{nc} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{c}\PY{p}{)} \PY{o}{*} \PY{n}{x}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}  \PY{c+c1}{\PYZsh{} dim = (n, l)}
     
    \PY{c+c1}{\PYZsh{} argmax p(class|data) = argmax log(p(class|data))}
    \PY{n}{pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{pred}
\end{Verbatim}
\end{tcolorbox}

    Now, you will write a function which calculates the error of a
classifier using the \(0,1\)-loss by comparing the true labels and the
predicted labels.

The function will take in two parameters: * A vector of length \(N\)
with the true labels as a numpy.ndarray (truelabels) * A vector of
length \(N\) with the estimated labels as a numpy.ndarray
(estimatedlabels)

The function will return the error (a scalar).

A function prototype is provided below (5 points):

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{classifierError}\PY{p}{(}\PY{n}{truelabels}\PY{p}{,}\PY{n}{estimatedlabels}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Put your code here}
    \PY{c+c1}{\PYZsh{} 0\PYZhy{}1 loss err = \PYZsh{} of misclassified labels / total number of labels}
    \PY{c+c1}{\PYZsh{} number of misclassified labels}
    \PY{n}{misclassified} \PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{truelabels} \PY{o}{!=} \PY{n}{estimatedlabels}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} classifier loss}
    \PY{k}{return} \PY{n}{misclassified} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{truelabels}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Now, we will load some sample data, in the format specified above. We
have three classes, with
\[\pi_0=\frac{1}{4}, \pi_1=\frac{1}{4}, \pi_2=\frac{1}{2}\]

\[\mathbf{\mu}_0=\begin{bmatrix} 1 \\ 5\end{bmatrix},\mathbf{\mu}_1=\begin{bmatrix} 5 \\ 0\end{bmatrix}, \mathbf{\mu}_2=\begin{bmatrix} -2\\-2\end{bmatrix} \]

\[\Sigma=\begin{bmatrix} 5 & 1 \\ 1 & 5 \end{bmatrix}\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} The prior information}
\PY{n}{pi}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{means}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{cov}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{} The data which you will use to test the classifier is called \PYZdq{}data\PYZdq{}}
\PY{n}{data}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{valdata}\PY{p}{)}
\PY{c+c1}{\PYZsh{} The labels are in \PYZdq{}truelabels\PYZdq{}}
\PY{n}{truelabels}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{vallabels}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Let's visualize the data by class. Each class will be in a different
color.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{scatter}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{truelabels}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(-7.345781630824372, 11.62528163082437, -8.5409666004415, 12.0151666004415)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Run the classifier on the data in \texttt{data} with labels
\texttt{truelabels}. Store the predicted labels in a variable called
\texttt{estimatedlabels} and report the classifier's error rate. Also,
run the classifier on the data in \texttt{drdata} and store the labels
outputted by the classifier into a variable called \texttt{drB}. We will
use \texttt{drB} to help visualize the decision regions. (5 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Put your code here.}
\PY{n}{estimatedlabels} \PY{o}{=} \PY{n}{bayesClassifier}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{pi}\PY{p}{,}\PY{n}{means}\PY{p}{,}\PY{n}{cov}\PY{p}{)}
\PY{n}{error} \PY{o}{=} \PY{n}{classifierError}\PY{p}{(}\PY{n}{truelabels}\PY{p}{,}\PY{n}{estimatedlabels}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Validation (Classification) Error: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{error}\PY{p}{)}\PY{p}{)}

\PY{n}{drB} \PY{o}{=} \PY{n}{bayesClassifier}\PY{p}{(}\PY{n}{drdata}\PY{p}{,} \PY{n}{pi}\PY{p}{,} \PY{n}{means}\PY{p}{,} \PY{n}{cov}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Validation (Classification) Error: 0.09583333333333334

    \end{Verbatim}

    \textbf{{[}Classification Error: 0.09583 or 9.583 \%{]}}

    Now, lets visualize the output of our classifier.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drB}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{truelabels}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.collections.PathCollection at 0x1c7c37b9448>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    You should observe linear boundaries between the decision regions and
almost all the points are in the correct region for this problem.

    \hypertarget{problem-2-linear-discriminant-analysis-25-points}{%
\paragraph{Problem 2 : Linear Discriminant Analysis (25
points)}\label{problem-2-linear-discriminant-analysis-25-points}}

    In this problem, you will implement Linear Discriminant Analysis (LDA).
You will use the Bayes classifier from Problem 1 to do this. You will be
given: * Training data feature vectors as a \((N,d)\) numpy.ndarray
(trainfeat), where each row is a feature vector. * Training data labels
as a length \(N\) numpy.ndarray (trainlabel)

The first function you will write will return a tuple of the estimates
of the prior probabilities (as a \(M\) length numpy.ndarray), means (as
a \((M,d)\) numpy.ndarray) and covariance matrix (as a \((d,d)\)
numpy.ndarray) in the LDA model. You may assume that labels
\(0,\ldots,\)trainlabel.max() exist in order to avoid some error
checking.

A hint: * You can use logical operations+slicing to index an array. For
example, if you want to get all training feature vectors whose labels
are \texttt{i}, you can use \texttt{trainfeat{[}trainlabel==i{]}}

A function prototype is provided below: (10 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{trainLDA}\PY{p}{(}\PY{n}{trainfeat}\PY{p}{,}\PY{n}{trainlabel}\PY{p}{)}\PY{p}{:}
    \PY{n}{nlabels}\PY{o}{=}\PY{n+nb}{int}\PY{p}{(}\PY{n}{trainlabel}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{}Assuming all labels up to nlabels exist.}
    \PY{n}{pi}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{nlabels}\PY{p}{)} \PY{c+c1}{\PYZsh{} store your prior in here}
    \PY{n}{n}\PY{p}{,} \PY{n}{d} \PY{o}{=} \PY{n}{trainfeat}\PY{o}{.}\PY{n}{shape} \PY{c+c1}{\PYZsh{} total number of samples and dimensions}
    \PY{n}{means}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{nlabels}\PY{p}{,}\PY{n}{d}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} store the class means in here}
    \PY{n}{cov}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{d}\PY{p}{,}\PY{n}{d}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} store the covariance matrix in here}
    
    \PY{c+c1}{\PYZsh{} Put your code here    }
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{nlabels}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} I(yi == i) * xi}
        \PY{n}{train\PYZus{}i} \PY{o}{=} \PY{n}{trainfeat}\PY{p}{[}\PY{n}{trainlabel} \PY{o}{==} \PY{n}{i}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} conpute priors}
        \PY{n}{pi}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{trainlabel}\PY{p}{[}\PY{n}{trainlabel} \PY{o}{==} \PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{n} 

        \PY{c+c1}{\PYZsh{} compute means}
        \PY{n}{means}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{train\PYZus{}i}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} compute covariance matrix}
        \PY{c+c1}{\PYZsh{} x = data \PYZhy{} means (with distinct labels)}
        \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{subtract}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{n}{train\PYZus{}i}\PY{p}{,} \PY{n}{train\PYZus{}i}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{means}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        \PY{n}{cov} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{cov}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} normalize covariance matrix}
    \PY{n}{cov} \PY{o}{=} \PY{n}{cov} \PY{o}{/} \PY{n}{n}

    \PY{k}{return} \PY{p}{(}\PY{n}{pi}\PY{p}{,}\PY{n}{means}\PY{p}{,}\PY{n}{cov}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Our training data is in a numpy array called \texttt{trainingdata}, with
corresponding labels \texttt{traininglabels}. Our validation data is in
a numpy array called \texttt{valdata}, with corresponding labels
\texttt{vallabels}. The data format is the same as Problem 1.

    And we can visualize the training data:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We will use the following code to visualize the decision regions of the classifier.}
\PY{c+c1}{\PYZsh{} You don\PYZsq{}t need to look at this cell.}

\PY{n}{scatter}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{traininglabels}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(-10.925541630824373, 10.317241630824372, -9.638406600441503, 11.7238066004415)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Use the bayesClassifier function you wrote in Problem 1 along with the
trainLDA function from Problem 2 to implement the LDA classifier. Train
the LDA classifier on the training data in \texttt{trainingdata}, and
then run the LDA classifier on the training data and the validation
data. Store the predicted training labels in
\texttt{estimatedtraininglabels} and the predicted labels on the
validation data in \texttt{estimatedvallabels}. Print out the prior,
means and covariance estimated in LDA. (And don't forget to run your LDA
classifier on the data in \texttt{drdata} and store the resultant
predicted labels in \texttt{drLDA} to help visualize the output of the
classifier.) (5 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Put your code here}
\PY{c+c1}{\PYZsh{} training LDA classifier}
\PY{n}{pi}\PY{p}{,} \PY{n}{means}\PY{p}{,} \PY{n}{cov} \PY{o}{=} \PY{n}{trainLDA}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{,} \PY{n}{traininglabels}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{prior: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{means: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{covariance: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{pi}\PY{p}{,} \PY{n}{means}\PY{p}{,} \PY{n}{cov}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} classifing training data}
\PY{n}{estimatedtraininglabels} \PY{o}{=} \PY{n}{bayesClassifier}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{,} \PY{n}{pi}\PY{p}{,} \PY{n}{means}\PY{p}{,} \PY{n}{cov}\PY{p}{)}
\PY{c+c1}{\PYZsh{} validating LDA classifier}
\PY{n}{estimatedvallabels} \PY{o}{=} \PY{n}{bayesClassifier}\PY{p}{(}\PY{n}{valdata}\PY{p}{,} \PY{n}{pi}\PY{p}{,} \PY{n}{means}\PY{p}{,} \PY{n}{cov}\PY{p}{)}

\PY{c+c1}{\PYZsh{} drLDA}
\PY{n}{drLDA} \PY{o}{=} \PY{n}{bayesClassifier}\PY{p}{(}\PY{n}{drdata}\PY{p}{,} \PY{n}{pi}\PY{p}{,} \PY{n}{means}\PY{p}{,} \PY{n}{cov}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
prior:
 [0.25 0.25 0.5 ]

means:
 [[ 1.12191279  5.17315581]
 [ 4.91358665 -0.28614234]
 [-2.07276858 -2.11161245]]

covariance:
 [[4.65175173 1.03950119]
 [1.03950119 4.76683419]]
    \end{Verbatim}

    The training data is generated with the distribution used in Problem 1,
so your \(\pi, \mu, {\sf C}\) should all be pretty close to the ones
given in Problem 1. If they are not close, you've done something wrong.

    Now, calculate the errors. Report the training error (error of the
classifier on the training data) and the validation error (error of the
classifier on the validation data). (5 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Put your code here}
\PY{n}{train\PYZus{}error} \PY{o}{=} \PY{n}{classifierError}\PY{p}{(}\PY{n}{traininglabels}\PY{p}{,} \PY{n}{estimatedtraininglabels}\PY{p}{)}
\PY{n}{val\PYZus{}error} \PY{o}{=} \PY{n}{classifierError}\PY{p}{(}\PY{n}{vallabels}\PY{p}{,} \PY{n}{estimatedvallabels}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training Error: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{train\PYZus{}error}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation Error: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{val\PYZus{}error}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Training Error: 0.07333333333333333
Validation Error: 0.10833333333333334
    \end{Verbatim}

    \textbf{{[}Training Error = 0.07333 or 7.333 \% and Validation Error =
0.10833 or 10.833 \%{]}}

    We can also visualize the performance of the classifier on the training
and validation data. In this problem, both the training and validation
data was generated from the distributions specified in Problem 1, so we
show both the LDA classifier (which you learned from the data) and the
Bayes classifier (which assumed you knew the true joint distribution of
the data and the labels).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} If this is looking a bit squished, you can change the 8 (width) and 8 (height)}
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drLDA}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{traininglabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data (LDA)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drB}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{traininglabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data (Bayes)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drLDA}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{vallabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Data (LDA)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drB}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{vallabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Data (Bayes)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Most of the points should be correctly classified in both the training
and validation data. If they are not, you've done something wrong.

    In this problem, we see that the LDA classifier gives boundaries which
are quite similar to the Bayes classifier (assuming you've implemented
both correctly). If you had a lot of training data from an arbitrary
distribution, would you expect the LDA classifier to give similar
boundaries to the Bayes classifier? Why or Why not? (5 points)

    \textbf{{[}When data (probability density functions) is normally
distributed, LDA converges to an optimal solution, that minimizes the
generalization error. When data is arbitrary and non-gaussian, the
solution may not be optimal, and hence, LDA classifier might not give
similar boundaries as the Bayes classifier.{]}}

    \hypertarget{problem-3-k-nearest-neighbors-some-short-answer-questions-35-points}{%
\paragraph{Problem 3: k-Nearest Neighbors + Some Short Answer Questions
(35
points)}\label{problem-3-k-nearest-neighbors-some-short-answer-questions-35-points}}

    In this problem, you will implement the k-Nearest Neighbors algorithm.

The following imports are copied from the beginning for your benefit.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{spatial}\PY{n+nn}{.}\PY{n+nn}{distance} \PY{k}{as} \PY{n+nn}{dist}
\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}
\end{Verbatim}
\end{tcolorbox}

    Your function will take: * Training data feature vectors as a \((N,d)\)
numpy.ndarray (trainfeat), where each row is a feature vector * Training
data labels as a length \(N\) numpy.ndarray (trainlabel) * Test data
feature vectors as a \((V,d)\) numpy.ndarray (testfeat), where each row
is a feature vector * The value of k

Use the Euclidean distance (scipy.spatial.distance.cdist) as your
dissimilarity measure. Read the documentation!

Your function should return a length \(V\) numpy.ndarray vector of the
estimated labels. This should take around 4 lines of code. Do not use
the kNN implementation in scikit-learn or similar.

Some functions which may be useful (read the documentation!): * The
numpy.argpartition function can be used to find the \(k\) smallest
elements of an array (via slicing) * scipy.stats.mode can find the most
common element in an array. Check the output.

(10 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{kNN}\PY{p}{(}\PY{n}{trainfeat}\PY{p}{,}\PY{n}{trainlabel}\PY{p}{,}\PY{n}{testfeat}\PY{p}{,}\PY{n}{k}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{}Put your code here}
    \PY{c+c1}{\PYZsh{} euclidean distances between all training and test points}
    \PY{n}{distances} \PY{o}{=} \PY{n}{dist}\PY{o}{.}\PY{n}{cdist}\PY{p}{(}\PY{n}{testfeat}\PY{p}{,} \PY{n}{trainfeat}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{euclidean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} indices of k nearest neighbours}
    \PY{n}{idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argpartition}\PY{p}{(}\PY{n}{distances}\PY{p}{,} \PY{n}{k}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} k nearest neighbours labels}
    \PY{n}{nn} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{take\PYZus{}along\PYZus{}axis}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{broadcast\PYZus{}to}\PY{p}{(}\PY{n}{trainlabel}\PY{p}{,} \PY{n}{distances}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{idx}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n}{k}\PY{p}{]}\PY{o}{.}\PY{n}{T}
    
    \PY{c+c1}{\PYZsh{} return the most frequent label}
    \PY{k}{return} \PY{n}{stats}\PY{o}{.}\PY{n}{mode}\PY{p}{(}\PY{n}{nn}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Run your k-Nearest Neighbors classifier with the training data in
\texttt{trainingdata} and validation data in \texttt{valdata} from
Problem 2, for \(k=1,3,4,5\). Compute the training and validation error
rates on the data from Problem 2. (5 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Put your code here}
\PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{:}
    \PY{n}{estimatedtraininglabels} \PY{o}{=} \PY{n}{kNN}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{,} \PY{n}{traininglabels}\PY{p}{,} \PY{n}{trainingdata}\PY{p}{,} \PY{n}{k}\PY{p}{)}
    \PY{n}{estimatedvallabels} \PY{o}{=} \PY{n}{kNN}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{,} \PY{n}{traininglabels}\PY{p}{,} \PY{n}{valdata}\PY{p}{,} \PY{n}{k}\PY{p}{)}
    \PY{n}{train\PYZus{}error} \PY{o}{=} \PY{n}{classifierError}\PY{p}{(}\PY{n}{traininglabels}\PY{p}{,} \PY{n}{estimatedtraininglabels}\PY{p}{)}
    \PY{n}{val\PYZus{}error} \PY{o}{=} \PY{n}{classifierError}\PY{p}{(}\PY{n}{vallabels}\PY{p}{,} \PY{n}{estimatedvallabels}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k= }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ Training Error: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{train\PYZus{}error}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Validation Error: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{val\PYZus{}error}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
k= 1:
 Training Error: 0.0
 Validation Error: 0.1125

k= 3:
 Training Error: 0.056666666666666664
 Validation Error: 0.10833333333333334

k= 4:
 Training Error: 0.06333333333333334
 Validation Error: 0.10833333333333334

k= 5:
 Training Error: 0.06
 Validation Error: 0.1

    \end{Verbatim}

    Which value of k would you choose for the k-NN classifier? Why? Run your
k-NN classifier with the chosen value of k on the data in
\texttt{drdata} and store the result in a variable called \texttt{drK}.
(5 points)

    \textbf{{[}I chose k = 5 because KNN with k = 5 has the least validation
error (and low training error as well). Low validation error is makes a
model generalize well.{]}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{k} \PY{o}{=} \PY{l+m+mi}{5}
\PY{n}{drK} \PY{o}{=} \PY{n}{kNN}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{,}\PY{n}{traininglabels}\PY{p}{,}\PY{n}{drdata}\PY{p}{,}\PY{n}{k}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Now, let us visualize the decision boundaries of your chosen value of
\(k\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} If this is looking a bit squished, you can change the 8 (width) and 8 (height)}
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drK}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{traininglabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data (}\PY{l+s+si}{\PYZpc{}i}\PY{l+s+s1}{\PYZhy{}NN)}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}}\PY{k}{k})
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drB}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{trainingdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{traininglabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data (Bayes)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drK}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{vallabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Data (}\PY{l+s+si}{\PYZpc{}i}\PY{l+s+s1}{\PYZhy{}NN)}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}}\PY{k}{k})
\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,}\PY{n}{yy}\PY{p}{,}\PY{n}{drB}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{n}{antialiased}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{scatter}\PY{p}{(}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{valdata}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{n}{vallabels}\PY{p}{)}
\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation Data (Bayes)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Some Short Answer Questions}

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  The training and validation data used in problems 1-3 was all drawn
  from the distribution described in problem 1. Compare and contrast the
  results you got from LDA and k-Nearest Neighbors as well as the Bayes
  classifier from problem 1. Your answer should consider both
  computational resources as well as error performance. (5 points)
\end{enumerate}

    \textbf{{[}The Bayes classifier was the most accurate of the 3
classifiers, and uses less computational resources than kNN classifier.
The 5-NN algorithm is the second most accurate, but uses the most
computational resources since it has to find all the nearest neighbours
for all the data samples. LDA uses slightly more computational resources
than Bayes classifier due to the estimation portion of the training step
(but less than kNN classifier), but it is less accurate than kNN
classifier for k = 5{]}}

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Compared to other values of k in k-NN, why the training error is zero
  when you use the standard nearest neighbor (1-NN) algorithm? (5
  points)
\end{enumerate}

    \textbf{{[}1-NN algorithm looks for 1 nearest neighbour. It finds
patterns in local data clusters (with 1 neighbour) and has high
variance. 1-NN tries to fit the training data perfectly. Hence, the
training error is zero{]}}

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Assuming you did not have knowledge of the true distribution of the
  data, out of the classifiers discussed in problems 1-3 (Bayes, LDA,
  kNN with the k you selected above), which classifier would you prefer
  in this problem? Why? (5 points)
\end{enumerate}

    \textbf{{[}Bayes Classifier finds the optimal solution, with least
generalization error, but we need to know the priors. Similarly, LDA
classifier assumes data is normally-distributed and hence, may not give
the optimal solution for arbitrary and unknown non-gaussian
distributions. But, kNN classifier does not assume the distribution of
the data and kNN with suitable k (here k = 5) has low generalization
error as well. Therefore, I would chose kNN classifier with suitable k,
if true distribution of data is unknown{]}}

    \hypertarget{problem-4-lda-and-knn-using-scikit-learn-20-points}{%
\paragraph{Problem 4: LDA and kNN using scikit-learn (20
points)}\label{problem-4-lda-and-knn-using-scikit-learn-20-points}}

    In many cases, you will be using other people's libraries to implement
learning algorithms. In this problem, you will become familiar with
scikit-learn's implementation of LDA and kNN.

First, we will load a data set of digits drawn from zip codes written on
US mail. This data set was designed to help get good algorithms to sort
mail by zip code automatically. It has been preprocessed a bit, with
details given here. Each feature vector consists of \(16^2\) real values
representing grayscale values of a 16 by 16 image of a digit. The
training data has 7291 samples, while the validation data has 2007
samples. Note that this is not the same dataset built into scikit-learn
-- it is much larger.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Loading the Data}

\PY{c+c1}{\PYZsh{}Read in the Training Data}
\PY{n}{traindata\PYZus{}tmp}\PY{o}{=} \PY{n}{genfromtxt}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{zip.train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 

\PY{c+c1}{\PYZsh{}The training labels are stored in \PYZdq{}trainlabels\PYZdq{}, training features in \PYZdq{}traindata\PYZdq{}. Rows are feature vectors.}
\PY{n}{trainlabels}\PY{o}{=}\PY{n}{traindata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{traindata}\PY{o}{=}\PY{n}{traindata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}

\PY{c+c1}{\PYZsh{}Read in the Validation Data}
\PY{n}{valdata\PYZus{}tmp}\PY{o}{=} \PY{n}{genfromtxt}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{zip.val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 

\PY{c+c1}{\PYZsh{}The validation labels are stored in \PYZdq{}vallabels\PYZdq{}, validation features in \PYZdq{}valdata\PYZdq{}. Rows are feature vectors.}
\PY{n}{vallabels}\PY{o}{=}\PY{n}{valdata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{valdata}\PY{o}{=}\PY{n}{valdata\PYZus{}tmp}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Use scikit-learn's sklearn.neighbors.KNeighborsClassifier to run a
Nearest Neighbor classifier (1-NN) on the validation data with the
provided training set. Note that KNeighborsClassifier defaults to 5-NN.

Measure the time for fitting the model and classification (the \%timeit
feature or time() or similar will be useful). Try the different
algorithms possible to fit the model (ball tree, kd-tree and brute
force, and specify the fastest one in your code). Make sure to calculate
the error on the validation set. (5 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{neighbors}
\PY{c+c1}{\PYZsh{} Insert code here}
\PY{n}{algorithms} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ball\PYZus{}tree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kd\PYZus{}tree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{brute}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{k}{for} \PY{n}{algorithm} \PY{o+ow}{in} \PY{n}{algorithms}\PY{p}{:}
    \PY{n}{neigh} \PY{o}{=} \PY{n}{neighbors}\PY{o}{.}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{algorithm}\PY{o}{=}\PY{n}{algorithm}\PY{p}{,} \PY{n}{metric}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{euclidean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Algorithm: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{algorithm}\PY{p}{)}\PY{p}{)}
    \PY{n}{start\PYZus{}time} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
    \PY{n}{neigh}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{traindata}\PY{p}{,} \PY{n}{trainlabels}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fitting time: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ s}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}time}\PY{p}{)}\PY{p}{)} 
    
    \PY{n}{start\PYZus{}time} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
    \PY{n}{estimatedvallabelsknn} \PY{o}{=} \PY{n}{neigh}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{valdata}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Classifing time: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ s}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}time}\PY{p}{)}\PY{p}{)} 
    
    \PY{n}{val\PYZus{}error\PYZus{}knn} \PY{o}{=} \PY{n}{classifierError}\PY{p}{(}\PY{n}{vallabels}\PY{p}{,} \PY{n}{estimatedvallabelsknn}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Validation Error: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{val\PYZus{}error\PYZus{}knn}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Algorithm: ball\_tree
Fitting time: 0.851567268371582 s
Classifing time: 10.784621477127075 s
Validation Error: 0.05630293971101146

Algorithm: kd\_tree
Fitting time: 1.4787938594818115 s
Classifing time: 14.427793741226196 s
Validation Error: 0.05630293971101146

Algorithm: brute
Fitting time: 0.012990474700927734 s
Classifing time: 0.6297745704650879 s
Validation Error: 0.05630293971101146

    \end{Verbatim}

    \textbf{{[}Fitting: 0.013 s and Classifying: 0.630 s and Validation
Error = 0.05630 or 5.630 \%{]}}

    Now, run LDA on the validation data set with scikit-learn's
sklearn.discriminant\_analysis.LinearDiscriminantAnalysis class. Measure
the training time as well as the time used to classify the validation
set. Make sure to calculate the error on the validation set. (5 points)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{discriminant\PYZus{}analysis} \PY{k+kn}{import} \PY{n}{LinearDiscriminantAnalysis}
\PY{c+c1}{\PYZsh{} Insert code here}
\PY{n}{clf} \PY{o}{=} \PY{n}{LinearDiscriminantAnalysis}\PY{p}{(}\PY{p}{)}

\PY{n}{start\PYZus{}time} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{traindata}\PY{p}{,} \PY{n}{trainlabels}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fitting: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ s}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}time}\PY{p}{)}\PY{p}{)}

\PY{n}{start\PYZus{}time} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{estimatedvallabelslda} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{valdata}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Classifing: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ s}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start\PYZus{}time}\PY{p}{)}\PY{p}{)}

\PY{n}{val\PYZus{}error\PYZus{}lda} \PY{o}{=} \PY{n}{classifierError}\PY{p}{(}\PY{n}{vallabels}\PY{p}{,} \PY{n}{estimatedvallabelslda}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Validation Error: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{val\PYZus{}error\PYZus{}lda}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Fitting: 0.5874032974243164 s
Classifing: 0.0043065547943115234 s

Validation Error: 0.114598903836572
    \end{Verbatim}

    \textbf{{[}Fitting: 0.587 s and Classifying: 0.004 s and Validation
Error = 0.11459 or 11.459 \%{]}}

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Based on the performance on the validation set, which algorithm would
  you pick (for this particular problem)? Your answer should also take
  into account computational resources required, error on the validation
  set, and the cost associated with making an error (in real life --
  recall the source of the data). (5 points)
\end{enumerate}

    \textbf{{[}I would pick the `brute-force' k-NN algorithm because it has
a lower validation error (0.05630 or 5.630 \%) and takes an around 0.643
seconds to train \& classify (validation step). Comparatively, LDA,
takes an around 0.592 seconds to train \& classify, but has higher
validation error (0.11459 or 11.459 \%). Therefore, eventhough, the
`brute-force' k-NN uses slighty costlier in terms of computational
resources, it will have a lower `real world' cost associated with making
an error because it has a lower generalization lower (about 50 \% lower)
and hence, less resources (people, gasoline, time) will be wasted{]}}

    \begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Do you think the 0,1-loss is appropriate error measure in this case?
  Why or why not? How can you use domain-specific knowledge to help
  improve performance for this application?
\end{enumerate}

If you are interested in this in more detail on this problem, see O.
Matan et al., ``Reading Handwritten Digits: A ZIP Code Recognition
System'', IEEE Computer, Vol 25, Number 7, pp 59-63, 1992 (tech report
version here). You do not need to look at this to answer the question.
(5 points)

    \textbf{{[}0-1 loss is an appropriate error measure because the
classification, in this case, is binary. The model either sorts the
mails correctly or it doesn't. Given the task of sorting mail by zip
code by reading handwritten digits, the performance of this application
could be improved by easing the process of recollection; potentially by
caching common features. For example, by using an `online' neural
network implementation, we could improve the speed and accuracy of
classification since the model will have to process smaller batches of
data rather than the full dataset.{]}}

    \hypertarget{and-this-concludes-lab-2-congratulations}{%
\subsection{And this concludes Lab 2!
Congratulations!}\label{and-this-concludes-lab-2-congratulations}}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
